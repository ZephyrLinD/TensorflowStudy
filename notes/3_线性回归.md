# 用Tensorflow实现线性回归

- 先我们需要用到: 
    ```
    import tensorflow as tf
    import numpy as np
    import matplotlib.pyplot as plt
    ```

- 使用numpy生成200个随机点和干扰项
    ```
    x_data = np.linspace(-0.5, 0.5, 200)[:,np.newaxis]      # 生成200个-0.5~0.5之间的均匀分布的点，并增加设成二维（x_data为200行1列）
    noise = np.random.normal(0, 0.02, x_data.shape)         # 干扰项，形状同x_data
    y_data = np.square(x_data) + noise
    ```

- 定义两个占位符

    ```
    x = tf.placeholder(tf.float32, [None, 1])               #x, y 的形状根据样本定义
    y = tf.placeholder(tf.float32, [None, 1])
    ```

- 定义神经网络中间层
    ```
    Weights_L1 = tf.Variable(tf.random_normal([1, 10])      # 权值(y = kx + b的k)
    biases_L1 = tf.Variable(tf.zeros([1, 10]))              # 偏置值(y = kx + b的b)
    Wx_plus_b_L1 = tf.matmul(x, Weights_L1) + biases_L1     # 求出信号的总合
    L1 = tf.nn.tanh(Wx_plus_b_L1)                           # 激活函数（双曲正切函数作用域信号的总合）
    ```

- 定义神经网络输出层
    ```
    Weights_L2 = tf.Varibale(tf.random_normal([10, 1]))
    biases_L2 = tf.Variable(tf.zeros[1, 1])
    Wx_plus_b_L2 = tf.matmul(L1, Weights_L2) + biases_L2
    prediction = tf.nn.tanh(Wx_plus_b_L2)
    ```

- 定义二次代价函数，并且使用梯度下降法
    ```
    loss = tf.reduce_mean(tf.square(y - prediction))
    train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)

- 会话
    ```
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        for _ in range(2000):
            sess.run(train_step, feed_dict={x:x_data, y:y_data})

        prediction_value = sess.run(prediction, feed_dict(x:x_data))    # 获得预测值
        # 画图
        plt.figure()
        plt.scatter(x_data, y_data)                                   # 散点图打印样本值
        plt.plot(x_data, prediction_value, 'r-', lw=6)                # 预测结果，红色实线， 线宽为6
        plt.show()

    ```
    